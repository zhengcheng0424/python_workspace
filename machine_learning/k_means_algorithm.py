"""
1. **定义**
   - K-Means算法是一种基于划分的聚类算法，它的目的是将给定的数据集划分成K个不同的簇（cluster）。
   这里的K是一个预先指定的参数，表示要将数据划分成的簇的数量。每个簇有一个中心点，称为质心（centroid）。
   算法通过不断地调整数据点所属的簇和簇的质心，使得同一簇内的数据点相似度尽可能高，不同簇之间的数据点相似度尽可能低。
2. **工作流程**
   - **初始化**：
     - 首先需要确定聚类的数目K。然后从数据集中随机选择K个数据点作为初始质心。
     例如，假设有一个二维平面上的数据集（像顾客在购物金额和购物频率两个维度上的数据），如果K = 3，就随机挑选3个点作为这3个簇的初始中心。
   - **分配数据点到簇**：
     - 对于数据集中的每个数据点，计算它到K个质心的距离。距离的计算通常使用欧几里得距离
      在二维平面中，对于点((x_1, y_1))和((x_2, y_2))，欧几里得距离: (d = sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2}) 。
     然后将数据点分配到距离它最近的质心所在的簇。
     例如，一个顾客的购物金额和购物频率构成的数据点，计算它到3个初始质心的距离后，把这个顾客分配到距离最近的那个簇中，
     这个簇代表了具有相似购物行为的顾客群体。
   - **更新质心**：
     - 当所有数据点都被分配到簇后，重新计算每个簇的质心。质心的计算是通过求簇内所有数据点在每个维度上的平均值得到的。
     例如，对于一个二维的顾客数据簇，新的质心的横坐标是簇内所有顾客购物金额的平均值，纵坐标是簇内所有顾客购物频率的平均值。
   - **迭代**：
     - 重复上述的“分配数据点到簇”和“更新质心”步骤，直到质心不再发生显著变化（通常可以通过设定一个收敛阈值来判断，
     比如两次迭代质心位置的变化小于某个很小的值）或者达到最大迭代次数。
3. **优点**
   - **简单易懂**：它的原理和实现过程相对直观，容易理解和掌握。
   - **计算效率高**：在处理大规模数据集时，K - Means算法的计算速度相对较快，因为它主要涉及到距离计算和简单的平均值计算。
   - **可扩展性好**：能够有效地处理大量的数据，并且可以通过并行计算等方式进一步提高效率。
4. **缺点**
   - **需要预先指定K值**：在实际应用中，很难提前知道数据应该被划分成多少个簇。如果K值选择不当，会导致聚类结果不理想。
   例如，K值过大，会使每个簇包含的数据点过少，可能会把本来属于同一类的对象划分到不同的簇中；K值过小，则可能会把不同类型的数据点划分到同一个簇中。
   - **对初始质心敏感**：不同的初始质心选择可能会导致不同的聚类结果。
   例如，在顾客聚类的例子中，如果初始质心选择的是比较极端的顾客（如只购买了一次非常昂贵商品的顾客和购买了很多便宜商品的顾客），
   可能会得到与实际情况不符的聚类结果。
   - **无法处理非球形的簇**：它假设簇是球形的，对于形状不规则的簇（如月牙形的簇），K - Means算法可能无法得到很好的聚类效果。
"""

from sklearn.cluster import KMeans
from sklearn.datasets import make_blobs
import matplotlib.pyplot as plt

# 创建数据集
x, _ = make_blobs(n_samples=300, centers=4, random_state=42)

# 创建KMeans模型: K=4 分为4簇
kmeans = KMeans(n_clusters=4)

# 训练模型
kmeans.fit(x)

# 预测
labels = kmeans.predict(x)

# 可视化结果
print(f"x={x}")
plt.scatter(x[:, 0], x[:, 1], c=labels, cmap='viridis')
centers = kmeans.cluster_centers_
print(f"centers={centers}")
plt.scatter(centers[:, 0], centers[:, 1], c='red', marker='x')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.title('K-Means Clustering')
plt.show()
