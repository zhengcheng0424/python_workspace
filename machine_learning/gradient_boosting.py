"""
1. **定义与基本原理**
   - 梯度提升（Gradient Boosting）是一种基于决策树（通常是CART树，即分类与回归树）的集成学习算法，用于解决回归和分类问题。
   其核心思想是通过迭代地构建一系列弱学习器（通常是决策树），并将它们组合起来形成一个强学习器。
   每一个新的弱学习器都在前面弱学习器的残差（在回归问题中）或者伪残差（在分类问题中）基础上进行学习。
2. **工作流程（以回归为例）**
   - **初始化**：首先，使用训练数据拟合一个初始的简单模型，通常是一个常数模型，例如预测值是训练数据目标变量的均值。设训练数据集为(X = (x_1,x_2,...,x_n))，
   对应的目标变量为(Y=(y_1,y_2,...,y_n))，初始模型(F_0(x))的预测值为(y=1/n*(y_1+y_2+...+y_n))。
   - **迭代构建弱学习器**：
     - 计算残差：对于第(m)次迭代（(m = 1,2,...,M)，(M)是总的迭代次数），计算当前模型(F_m(x))的残差(r_m = y - F_{m - 1}(x))。
     残差表示了当前模型预测值与真实值之间的差异。
     - 构建弱学习器：基于残差(r_m)训练一个新的弱学习器(h_m(x))，例如一个决策树。决策树的构建目的是尽可能地拟合残差。
     - 更新模型：更新强学习器(F_m(x)=F_{m - 1}(x)+\nu h_m(x))，其中(\nu)是学习率，它控制了每次更新的步长，取值范围通常在((0,1])。
     学习率较小意味着模型更新更慢，但可能会有更好的泛化性能。
   - **最终模型**：经过(M)次迭代后，得到最终的模型(F_M(x))，它是由初始模型和一系列弱学习器组合而成的。
3. **在分类问题中的应用**
   - 在分类问题中，使用类似于逻辑回归中的对数似然损失函数的负梯度作为伪残差来训练弱学习器。
   例如，对于二分类问题，采用对数损失函数(L(y,F(x)) = -[yln(p)+(1 - y)ln(1 - p)])，其中(y)是真实类别（(0)或(1)），
   (p = \frac{1}{1+e^{-F(x)}})是预测为正类的概率。
   计算其负梯度作为伪残差来训练决策树，后续的迭代过程与回归问题类似，最后组合多个弱学习器来得到分类模型。
4. **优点**
   - **准确性高**：通过迭代地学习残差或伪残差，能够有效地减少模型的预测误差，在许多数据集上可以取得很高的预测准确性，尤其在处理复杂的非线性关系的数据时表现出色。
   - **灵活性**：可以灵活地处理各种类型的数据，包括数值型和分类型数据，并且可以通过调整参数（如学习率、迭代次数、树的深度等）来优化模型性能。
   - **能够处理缺失数据**：在构建决策树的过程中，可以通过一些策略（如划分节点时忽略缺失值样本或者使用替代划分等）来处理缺失数据，使得模型对数据质量的要求相对没有那么严格。
5. **缺点**
   - **容易过拟合**：如果迭代次数过多或者树的深度过深等，很容易导致过拟合，使得模型在训练数据上表现很好，但在新的数据上表现不佳。
   需要通过一些技术如早停止（Early Stopping）、调整学习率等来防止过拟合。
   - **计算成本较高**：由于是迭代地构建多个弱学习器，特别是当数据量较大、迭代次数较多或者决策树比较复杂时，训练时间会比较长，计算资源消耗也比较大。
   
   1. **目标函数与损失函数**
   - 在梯度提升中，我们的目标是最小化一个给定的损失函数(L(y,F(x)))，其中(y)是真实的目标值（对于回归问题是实数，对于分类问题是类别标签），
   (F(x))是我们要学习的模型对于输入(x)的预测值。
   例如，在回归问题中，常用的损失函数是均方误差（MSE） L(y,F(x)) = (y - F(x))^2；
   在二分类问题中，可能会使用对数损失函数  L(y,F(x)) = -[yln(p)+(1 - y)ln(1 - p)]  ，
   其中(p=1/(1 + e^(-F(x)))是预测为正类的概率。
2. **加法模型表示**
   - 梯度提升模型可以看作是一个加法模型，即 FM(x)=F0(x)+((\nu)*h1(x)+(\nu)*h2(x)+...+(\nu)*hM(x))，其中 F0(x) 是初始模型，
   (hm(x))是第(m)个弱学习器（通常是决策树），(\nu)是学习率，(M)是弱学习器的数量。
   - 初始模型(F0(x))通常是一个简单的模型，在回归问题中，(F0(x))可以是训练数据目标变量的均值，
   即 F0(x)=arg min(L(y1,c), L(y2,c), ... , L(yn,c))，δ
   其中(c)是一个常数，通过求解这个最小化问题得到(c = 1/n*(y1+y2+...+yn)。
3. **梯度下降与伪残差**
   - 梯度提升的关键思想是利用损失函数的负梯度方向来更新模型。在第(m)次迭代时，我们希望找到一个弱学习器(hm(x))，使得它能够沿着损失函数下降最快的方向更新模型。
   - 根据泰勒展开式，损失函数(L(y, Fm(x)))在(F_{m - 1}(x))附近的二阶近似可以表示为
   L(y,F_{m}(x)) ≈ L(y,F_{m - 1}(x)) + g_{m}(x)(F_{m}(x)-F_{m - 1}(x)) + 1/2*(h_{m}(x))(F_{m}(x)-F_{m - 1}(x))^2 ，
   其中:
   g_{m}(x) =  ∂L(y,F(x)) / ∂F(x) | {F(x)=F_{m - 1}(x)}  是损失函数在(F_{m - 1}(x))处的梯度，
   h_{m}(x) = ∂^{2}L(y,F(x)) /  F(x)^{2} | {F(x)=F_{m - 1}(x)} 是损失函数在(F_{m - 1}(x))处的二阶导数。
   - 为了最小化这个近似的损失函数，我们可以忽略二阶导数（在一些实现中会考虑，但为了简化先忽略），只关注梯度项。
   此时，我们希望新的弱学习器(h_{m}(x))能够近似拟合负梯度(-g_{m}(x))。
   在回归问题中，对于均方误差损失函数，(g_{m}(x) = y - F_{m - 1}(x))，这正好是残差。
   在分类问题中，根据不同的损失函数计算得到的负梯度被称为伪残差。
4. **更新模型**
   - 找到拟合负梯度（或伪残差）的弱学习器(h_{m}(x))后，我们按照(F_{m}(x)=F_{m - 1}(x)+\nu h_{m}(x))来更新模型。学习率(\nu)起到控制更新步长的作用。
   如果(\nu)太大，可能会导致模型过拟合；如果(\nu)太小，模型收敛会很慢。
   - 通过不断地迭代这个过程，每次都在之前模型的基础上，利用损失函数的梯度信息来更新模型，使得最终的模型能够尽可能地最小化损失函数，从而提高模型的预测性能。
5. **正则化方法**
   - 为了防止过拟合，梯度提升模型还会采用一些正则化方法。例如，对每棵决策树的复杂度进行限制，如限制树的深度、叶子节点的数量等；
   或者通过随机采样（如子采样）来构建每棵决策树，减少数据的过拟合风险；还可以对学习率(\nu)进行调整，找到一个合适的平衡，使得模型既能快速收敛又能有较好的泛化性能。

"""

from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.datasets import load_iris

# 创建数据集
data = load_iris()
x, y = data.data, data.target
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

# 创建GradientBoostingClassifier模型:
model = GradientBoostingClassifier()

# 训练模型
model.fit(x_train, y_train)

# 预测
predictions = model.predict(x_test)

# 结果
print(f"x = {x}, predictions = {predictions}")
accuracy = accuracy_score(y_test, predictions)
print(f"accuracy = {accuracy * 100:.2f} %")
