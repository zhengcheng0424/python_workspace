"""
1. **定义**
   - 主成分分析（Principal Component Analysis，PCA）是一种多变量统计分析方法。
   它的主要目的是通过线性变换将原始数据的多个变量（特征）转换为一组新的变量，即主成分。
   这些主成分是原始变量的线性组合，并且按照方差大小依次排列，第一个主成分包含了原始数据中方差最大的方向，
   第二个主成分在与第一个主成分正交（垂直）的方向上包含方差次大的方向，以此类推。
2. **数学原理**
   - 假设我们有一个数据集$X$，其中包含$n$个样本和$m$个特征，即$X$是一个$n\times m$的矩阵。
   PCA首先会计算数据的协方差矩阵（如果数据已经标准化，即均值为0，标准差为1，协方差矩阵可以很好地表示变量之间的相关性）。
   协方差矩阵$C$是一个$m\times m$的对称矩阵，其元素$C_{ij}$表示第$i$个特征和第$j$个特征之间的协方差。
   - 然后，通过求解协方差矩阵的特征值和特征向量来找到主成分。特征值代表了对应的特征向量方向上的方差大小。
   将特征值从大到小排序，对应的特征向量就是主成分的方向。例如，最大的特征值对应的特征向量就是第一个主成分的方向。
   - 对于一个新的数据点$x$（$x$是一个$m$维向量），它在第$i$个主成分上的投影（即主成分得分）可以通过$x$与第$i$个特征向量的内积来计算。
3. **应用场景**
   - **数据降维**：在实际数据处理中，当数据的维度（特征数量）很高时，可能会出现维度灾难的问题，例如计算复杂度增加、过拟合等。
   PCA可以将高维数据投影到低维空间，同时尽量保留数据的主要信息。例如，在图像识别中，一幅图像可能有成千上万个像素（特征），
   通过PCA可以将这些特征降维到一个较低的维度，从而加快后续的分类或识别算法的速度，同时也能减少存储空间。
   - **数据可视化**：将高维数据降到二维或三维空间，这样可以更直观地观察数据的分布和结构。比如，在分析一个包含多种化学物质成分的数据集时，
   原始数据可能有多个化学指标作为特征，通过PCA降到二维后，可以在平面上画出数据点的分布，从而更好地理解不同样本之间的关系。
4. **优势**
   - **简单有效**：PCA是一种相对简单的数学方法，不需要复杂的模型假设。它能够快速地对数据进行降维和特征提取，并且在很多情况下能够很好地保留数据的主要信息。
   - **无监督学习方法**：不依赖于数据的类别标签，适用于没有明确分类标签的数据挖掘和数据分析任务。
   例如，在探索性数据分析中，当还不清楚数据的类别划分时，PCA可以帮助分析数据的内在结构。
5. **局限性**
   - **线性方法**：PCA是基于线性变换的方法，对于一些数据中存在的非线性关系可能无法很好地处理。
   例如，在一些具有复杂曲线分布的数据集中，PCA可能无法准确地捕捉到数据的真实结构。
   - **信息损失**：虽然PCA在降维过程中尽量保留主要信息，但不可避免地会损失一定的信息。
   尤其是当降维程度较大时，一些次要的但可能在某些特定应用中有价值的信息会丢失。
   例如，在医学数据分析中，一些细微的疾病特征可能在降维过程中被忽略。
"""

from sklearn.decomposition import PCA
from sklearn.datasets import load_iris
import matplotlib.pyplot as plt

# 加载数据集
data = load_iris()
x = data.data
y = data.target

# 创建PCA模型
pca = PCA(n_components=2)

# 转换数据
x_pca = pca.fit_transform(x)

# 可视化结果
print(x_pca.shape, x_pca)
plt.scatter(x_pca[:, 0], x_pca[:, 1], c=y, cmap='viridis')
plt.xlabel('First Principal Component')
plt.ylabel('Second Principal Component')
plt.title('PCA of Iris Dataset')
plt.show()
